{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roll : 21226\n",
    "\n",
    "Assignment 6 : Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Model Parameters\n",
    "\n",
    "Training a model is iterative and each iteration, the model guess the output and then calculates the error and using that it optimizes the parameters.\n",
    "\n",
    "## Code\n",
    "\n",
    "We will be using our FashionMNIST dataset to perform model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5, 0, 9, 5, 5, 7, 9, 1, 0, 6, 4, 3, 1, 4, 8,\n",
       "         4, 3, 0, 2, 4, 4, 5, 3, 6, 6, 0, 8, 5, 2, 1, 6, 6, 7, 9, 5, 9, 2, 7, 3,\n",
       "         0, 3, 3, 3, 7, 2, 2, 6, 6, 8, 3, 3, 5, 0, 5, 5])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "These are adjustable parameters for training.\n",
    "\n",
    "**Learning Rate** is for letting the model know how many updates it must do of the model parameter at each epoch.\n",
    "\n",
    "**Batch Size** is to indicate the number of samples that go into the network before parameter updating.\n",
    "\n",
    "**Epochs** is the \\# of iterations over a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Loss is the parameter that we need to optimize during Model Training.\n",
    "\n",
    "To calculate the loss, we make a prediction using the input data and compare it against the actual lavbel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "It is used to adjust model parameters and reduce model error in each training iteration.\n",
    "\n",
    "Here we use Stochastic Gradient Descent as our optimizer. But there are other popular ones such as AdamW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    predicted_labels = []  # Store the predicted labels\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            predicted_labels.extend(pred.argmax(1).tolist())  # Add the predicted labels to the list\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    # Convert the list of predicted labels to a torch tensor\n",
    "    predicted_labels = torch.tensor(predicted_labels)\n",
    "    # Print actual vs. predicted labels\n",
    "    print(\"Actual Labels: \", dataloader.dataset.targets)\n",
    "    print(\"Predicted Labels: \", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we train our model for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.298682  [   64/60000]\n",
      "loss: 2.296704  [ 6464/60000]\n",
      "loss: 2.272354  [12864/60000]\n",
      "loss: 2.263237  [19264/60000]\n",
      "loss: 2.264405  [25664/60000]\n",
      "loss: 2.219508  [32064/60000]\n",
      "loss: 2.233826  [38464/60000]\n",
      "loss: 2.198060  [44864/60000]\n",
      "loss: 2.195070  [51264/60000]\n",
      "loss: 2.169406  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.2%, Avg loss: 2.158026 \n",
      "\n",
      "Actual Labels:  tensor([9, 2, 1,  ..., 8, 1, 5])\n",
      "Predicted Labels:  tensor([9, 2, 1,  ..., 4, 1, 8])\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.164098  [   64/60000]\n",
      "loss: 2.161208  [ 6464/60000]\n",
      "loss: 2.101996  [12864/60000]\n",
      "loss: 2.117267  [19264/60000]\n",
      "loss: 2.072944  [25664/60000]\n",
      "loss: 2.001401  [32064/60000]\n",
      "loss: 2.042844  [38464/60000]\n",
      "loss: 1.961602  [44864/60000]\n",
      "loss: 1.968178  [51264/60000]\n",
      "loss: 1.900620  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.892263 \n",
      "\n",
      "Actual Labels:  tensor([9, 2, 1,  ..., 8, 1, 5])\n",
      "Predicted Labels:  tensor([9, 2, 1,  ..., 3, 1, 9])\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.919899  [   64/60000]\n",
      "loss: 1.892006  [ 6464/60000]\n",
      "loss: 1.779712  [12864/60000]\n",
      "loss: 1.822299  [19264/60000]\n",
      "loss: 1.708566  [25664/60000]\n",
      "loss: 1.654215  [32064/60000]\n",
      "loss: 1.690936  [38464/60000]\n",
      "loss: 1.592273  [44864/60000]\n",
      "loss: 1.613515  [51264/60000]\n",
      "loss: 1.508213  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.518477 \n",
      "\n",
      "Actual Labels:  tensor([9, 2, 1,  ..., 8, 1, 5])\n",
      "Predicted Labels:  tensor([9, 2, 1,  ..., 3, 1, 9])\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.582684  [   64/60000]\n",
      "loss: 1.545916  [ 6464/60000]\n",
      "loss: 1.402406  [12864/60000]\n",
      "loss: 1.469322  [19264/60000]\n",
      "loss: 1.346611  [25664/60000]\n",
      "loss: 1.341855  [32064/60000]\n",
      "loss: 1.360872  [38464/60000]\n",
      "loss: 1.292391  [44864/60000]\n",
      "loss: 1.319780  [51264/60000]\n",
      "loss: 1.216476  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.239061 \n",
      "\n",
      "Actual Labels:  tensor([9, 2, 1,  ..., 8, 1, 5])\n",
      "Predicted Labels:  tensor([9, 2, 1,  ..., 3, 1, 7])\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.315363  [   64/60000]\n",
      "loss: 1.296673  [ 6464/60000]\n",
      "loss: 1.137833  [12864/60000]\n",
      "loss: 1.236779  [19264/60000]\n",
      "loss: 1.115705  [25664/60000]\n",
      "loss: 1.136344  [32064/60000]\n",
      "loss: 1.160700  [38464/60000]\n",
      "loss: 1.105773  [44864/60000]\n",
      "loss: 1.138879  [51264/60000]\n",
      "loss: 1.052536  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 1.070761 \n",
      "\n",
      "Actual Labels:  tensor([9, 2, 1,  ..., 8, 1, 5])\n",
      "Predicted Labels:  tensor([9, 2, 1,  ..., 3, 1, 7])\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.138769  [   64/60000]\n",
      "loss: 1.142694  [ 6464/60000]\n",
      "loss: 0.967953  [12864/60000]\n",
      "loss: 1.097708  [19264/60000]\n",
      "loss: 0.980787  [25664/60000]\n",
      "loss: 1.001867  [32064/60000]\n",
      "loss: 1.042523  [38464/60000]\n",
      "loss: 0.990380  [44864/60000]\n",
      "loss: 1.024208  [51264/60000]\n",
      "loss: 0.955297  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.966235 \n",
      "\n",
      "Actual Labels:  tensor([9, 2, 1,  ..., 8, 1, 5])\n",
      "Predicted Labels:  tensor([9, 2, 1,  ..., 3, 1, 7])\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.019729  [   64/60000]\n",
      "loss: 1.045999  [ 6464/60000]\n",
      "loss: 0.854593  [12864/60000]\n",
      "loss: 1.007829  [19264/60000]\n",
      "loss: 0.899105  [25664/60000]\n",
      "loss: 0.909485  [32064/60000]\n",
      "loss: 0.967991  [38464/60000]\n",
      "loss: 0.917026  [44864/60000]\n",
      "loss: 0.947162  [51264/60000]\n",
      "loss: 0.891788  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.896662 \n",
      "\n",
      "Actual Labels:  tensor([9, 2, 1,  ..., 8, 1, 5])\n",
      "Predicted Labels:  tensor([9, 2, 1,  ..., 3, 1, 7])\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.934043  [   64/60000]\n",
      "loss: 0.980258  [ 6464/60000]\n",
      "loss: 0.774432  [12864/60000]\n",
      "loss: 0.944961  [19264/60000]\n",
      "loss: 0.845509  [25664/60000]\n",
      "loss: 0.842744  [32064/60000]\n",
      "loss: 0.916646  [38464/60000]\n",
      "loss: 0.868532  [44864/60000]\n",
      "loss: 0.892040  [51264/60000]\n",
      "loss: 0.846290  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.847126 \n",
      "\n",
      "Actual Labels:  tensor([9, 2, 1,  ..., 8, 1, 5])\n",
      "Predicted Labels:  tensor([9, 2, 1,  ..., 8, 1, 7])\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.868846  [   64/60000]\n",
      "loss: 0.931418  [ 6464/60000]\n",
      "loss: 0.715033  [12864/60000]\n",
      "loss: 0.898188  [19264/60000]\n",
      "loss: 0.807408  [25664/60000]\n",
      "loss: 0.792960  [32064/60000]\n",
      "loss: 0.878130  [38464/60000]\n",
      "loss: 0.834983  [44864/60000]\n",
      "loss: 0.851068  [51264/60000]\n",
      "loss: 0.811540  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.809710 \n",
      "\n",
      "Actual Labels:  tensor([9, 2, 1,  ..., 8, 1, 5])\n",
      "Predicted Labels:  tensor([9, 2, 1,  ..., 8, 1, 7])\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.816895  [   64/60000]\n",
      "loss: 0.892168  [ 6464/60000]\n",
      "loss: 0.669090  [12864/60000]\n",
      "loss: 0.861911  [19264/60000]\n",
      "loss: 0.778329  [25664/60000]\n",
      "loss: 0.754807  [32064/60000]\n",
      "loss: 0.847237  [38464/60000]\n",
      "loss: 0.810249  [44864/60000]\n",
      "loss: 0.819377  [51264/60000]\n",
      "loss: 0.783371  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.779901 \n",
      "\n",
      "Actual Labels:  tensor([9, 2, 1,  ..., 8, 1, 5])\n",
      "Predicted Labels:  tensor([9, 2, 1,  ..., 8, 1, 7])\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have achieved a model accuracy of 70.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
