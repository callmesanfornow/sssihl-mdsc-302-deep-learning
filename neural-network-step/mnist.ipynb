{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Test Data Download\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sizes for train and validation sets\n",
    "train_size = int(0.9 * len(training_data))  \n",
    "val_size = len(training_data) - train_size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "train_set, val_set = random_split(training_data, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54000\n",
      "6000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(train_set.__len__())\n",
    "print(val_set.__len__())\n",
    "print(test_data.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Assuming you have already loaded the test_dataset as shown before\n",
    "\n",
    "# Choose the index of the sample you want to visualize\n",
    "sample_index = 0\n",
    "\n",
    "# Get the image and label at the specified index\n",
    "sample_image, sample_label = test_data[sample_index]\n",
    "\n",
    "# Create a transform to reverse the normalization applied earlier\n",
    "inverse_transform = transforms.Compose([transforms.Normalize((-0.5,), (2.0,)), transforms.ToPILImage()])\n",
    "\n",
    "# Convert the image tensor to a PIL image\n",
    "sample_image_pil = inverse_transform(sample_image)\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(sample_image_pil, cmap='gray')\n",
    "plt.title(f'Label: {sample_label}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten the input\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        output = F.softmax(self.fc3(x), dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (fc1): Linear(in_features=784, out_features=2048, bias=True)\n",
       "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 2048]       1,607,680\n",
      "            Linear-2                  [-1, 512]       1,049,088\n",
      "            Linear-3                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 2,661,898\n",
      "Trainable params: 2,661,898\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 6.12\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 10.15\n",
      "Estimated Total Size (MB): 16.30\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (28, 28, 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used :  cpu\n"
     ]
    }
   ],
   "source": [
    "# Selecting Device for training\n",
    "\n",
    "'''device = (\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else 'mps' if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")'''\n",
    "device = 'cpu'\n",
    "print('Device being used : ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.5236\n",
      "Epoch [2/10], Loss: 1.4888\n",
      "Epoch [3/10], Loss: 1.5004\n",
      "Epoch [4/10], Loss: 1.5423\n",
      "Epoch [5/10], Loss: 1.5028\n",
      "Epoch [6/10], Loss: 1.4820\n",
      "Epoch [7/10], Loss: 1.4820\n",
      "Epoch [8/10], Loss: 1.5445\n",
      "Epoch [9/10], Loss: 1.4612\n",
      "Epoch [10/10], Loss: 1.4612\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Assuming you have already loaded the test_dataset as shown before\n",
    "\n",
    "# Choose the index of the sample you want to visualize\n",
    "sample_index = 0\n",
    "\n",
    "# Get the image and label at the specified index\n",
    "sample_image, sample_label = test_data[sample_index]\n",
    "\n",
    "# Create a transform to reverse the normalization applied earlier\n",
    "inverse_transform = transforms.Compose([transforms.Normalize((-0.5,), (2.0,)), transforms.ToPILImage()])\n",
    "\n",
    "# Convert the image tensor to a PIL image\n",
    "sample_image_pil = inverse_transform(sample_image)\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(sample_image_pil, cmap='gray')\n",
    "plt.title(f'Label: {sample_label}')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def print_image(image, label):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 95.82%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
